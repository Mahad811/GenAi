{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Q1: CIFAR-10 CNN - Colab Runner\n",
        "\n",
        "This notebook sets up GPU, installs dependencies, runs ablation, trains the optimal model, evaluates on test set, visualizes feature maps, and zips outputs for download.\n",
        "\n",
        "- Make sure to pick GPU: Runtime → Change runtime type → GPU\n",
        "- Choose ONE of the three project setup paths below: Google Drive, GitHub clone, or Zip upload.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('Enable GPU in Runtime > Change runtime type')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Project setup: choose one path below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect environment (Colab vs Kaggle) and set base paths\n",
        "import os, sys, glob\n",
        "\n",
        "is_colab = 'google.colab' in sys.modules\n",
        "is_kaggle = bool(os.environ.get('KAGGLE_URL_BASE')) or os.path.exists('/kaggle/input')\n",
        "\n",
        "print('Environment:','Kaggle' if is_kaggle else ('Colab' if is_colab else 'Local'))\n",
        "\n",
        "BASE_DIR = '/content' if is_colab else ('/kaggle/working' if is_kaggle else os.getcwd())\n",
        "print('Base dir:', BASE_DIR)\n",
        "\n",
        "# Helper to try to locate the project automatically on Kaggle\n",
        "PROJECT_DIR = None\n",
        "Q1_DIR = None\n",
        "if is_kaggle:\n",
        "    candidates = []\n",
        "    # Prefer anything in /kaggle/working first\n",
        "    candidates += [p for p in [\n",
        "        '/kaggle/working/genai_A1',\n",
        "        '/kaggle/working/GenAi',\n",
        "        '/kaggle/working/genai',\n",
        "    ] if os.path.exists(p)]\n",
        "    # Search mounted input datasets for a folder that contains Q1/src\n",
        "    for root in glob.glob('/kaggle/input/*', recursive=False):\n",
        "        for p in [root, os.path.join(root, 'genai_A1'), os.path.join(root, 'GenAi')]:\n",
        "            q1 = os.path.join(p, 'Q1')\n",
        "            if os.path.exists(os.path.join(q1, 'src')):\n",
        "                candidates.append(p)\n",
        "    # Pick the shortest valid candidate\n",
        "    if candidates:\n",
        "        PROJECT_DIR = sorted(candidates, key=len)[0]\n",
        "        Q1_DIR = os.path.join(PROJECT_DIR, 'Q1')\n",
        "        print('Auto-detected PROJECT_DIR:', PROJECT_DIR)\n",
        "        print('Q1_DIR:', Q1_DIR)\n",
        "        os.chdir(Q1_DIR)\n",
        "        os.environ['PYTHONPATH'] = Q1_DIR\n",
        "        !pwd && ls -la\n",
        "    else:\n",
        "        print('Could not auto-detect project under /kaggle/input or /kaggle/working.')\n",
        "        print('If you attached a dataset, ensure it contains genai_A1/Q1 or GenAi/Q1.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle setup: copy project from /kaggle/input to /kaggle/working if needed\n",
        "import os, shutil, glob\n",
        "\n",
        "if is_kaggle:\n",
        "    def find_project_roots():\n",
        "        roots = []\n",
        "        # common names under working\n",
        "        for p in ['/kaggle/working/genai_A1', '/kaggle/working/GenAi', '/kaggle/working/genai']:\n",
        "            if os.path.exists(os.path.join(p, 'Q1', 'src')):\n",
        "                roots.append(p)\n",
        "        # search input datasets\n",
        "        for root in glob.glob('/kaggle/input/*', recursive=False):\n",
        "            for name in ['genai_A1', 'GenAi', 'genai']:\n",
        "                p = os.path.join(root, name)\n",
        "                if os.path.exists(os.path.join(p, 'Q1', 'src')):\n",
        "                    roots.append(p)\n",
        "            # Also allow when the dataset root itself contains Q1\n",
        "            if os.path.exists(os.path.join(root, 'Q1', 'src')):\n",
        "                roots.append(root)\n",
        "        return roots\n",
        "\n",
        "    roots = find_project_roots()\n",
        "    print('Detected project candidates:', roots)\n",
        "\n",
        "    # Choose a source under /kaggle/input if present; else prefer working\n",
        "    src = next((r for r in roots if r.startswith('/kaggle/input/')), None) or (roots[0] if roots else None)\n",
        "    if src is None:\n",
        "        print('No project found. Attach a Dataset with the repository and re-run.')\n",
        "    else:\n",
        "        dst = '/kaggle/working/genai_A1' if os.path.basename(src).lower() != 'genai_a1' else '/kaggle/working/genai_A1'\n",
        "        print('Source:', src)\n",
        "        print('Destination:', dst)\n",
        "        if not os.path.exists(dst):\n",
        "            print('Copying project to working...')\n",
        "            shutil.copytree(src, dst)\n",
        "        else:\n",
        "            print('Project already exists in working. Updating files (if any)...')\n",
        "            # minimal sync: copy src/Q1 over dst/Q1\n",
        "            for item in ['Q1', 'requirements.txt']:\n",
        "                s = os.path.join(src, item)\n",
        "                d = os.path.join(dst, item)\n",
        "                if os.path.exists(s):\n",
        "                    if os.path.isdir(s):\n",
        "                        shutil.copytree(s, d, dirs_exist_ok=True)\n",
        "                    else:\n",
        "                        shutil.copy2(s, d)\n",
        "        PROJECT_DIR = dst\n",
        "        Q1_DIR = os.path.join(PROJECT_DIR, 'Q1')\n",
        "        os.chdir(Q1_DIR)\n",
        "        os.environ['PYTHONPATH'] = Q1_DIR\n",
        "        print('Using PROJECT_DIR:', PROJECT_DIR)\n",
        "        print('Using Q1_DIR:', Q1_DIR)\n",
        "        !pwd && ls -la\n",
        "else:\n",
        "    print('Kaggle setup skipped (not running on Kaggle).')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Mount Google Drive and use existing project in Drive (Colab only)\n",
        "import os\n",
        "if is_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    PROJECT_DIR = '/content/drive/MyDrive/your_project_path/genai_A1'  # <-- EDIT THIS\n",
        "    Q1_DIR = os.path.join(PROJECT_DIR, 'Q1')\n",
        "    %cd $Q1_DIR\n",
        "    os.environ['PYTHONPATH'] = Q1_DIR\n",
        "    !pwd && ls -la\n",
        "else:\n",
        "    print('Skipping Google Drive mount (not Colab).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option B: Clone from GitHub (set path by environment)\n",
        "import os\n",
        "base = '/content' if is_colab else ('/kaggle/working' if is_kaggle else os.getcwd())\n",
        "%cd $base\n",
        "!git clone https://github.com/your/repo.git genai_A1\n",
        "%cd $base/genai_A1/Q1\n",
        "PROJECT_DIR = f\"{base}/genai_A1\"\n",
        "Q1_DIR = f\"{PROJECT_DIR}/Q1\"\n",
        "os.environ['PYTHONPATH'] = Q1_DIR\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option C: Upload or attach a ZIP and unzip to base dir\n",
        "import os\n",
        "base = '/content' if is_colab else ('/kaggle/working' if is_kaggle else os.getcwd())\n",
        "\n",
        "if is_colab:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # upload your project zip\n",
        "    zip_name = list(uploaded.keys())[0]\n",
        "    !unzip -o \"$zip_name\" -d $base\n",
        "else:\n",
        "    print('On Kaggle: attach a Dataset containing the repo ZIP or folder. If you have a local zip in working, set zip_name accordingly and run unzip manually:')\n",
        "    print('!unzip -o \"/kaggle/working/your_zip.zip\" -d /kaggle/working')\n",
        "\n",
        "%cd $base/genai_A1/Q1\n",
        "PROJECT_DIR = f\"{base}/genai_A1\"\n",
        "Q1_DIR = f\"{PROJECT_DIR}/Q1\"\n",
        "os.environ['PYTHONPATH'] = Q1_DIR\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "base = '/content' if is_colab else ('/kaggle/working' if is_kaggle else os.getcwd())\n",
        "req_path = f\"{base}/genai_A1/requirements.txt\"\n",
        "print('Using requirements:', req_path)\n",
        "%pip install -U pip\n",
        "%pip install -r \"$req_path\"\n",
        "\n",
        "# Optional: Speed up HF datasets cache\n",
        "import os\n",
        "if is_colab:\n",
        "    os.environ['HF_DATASETS_CACHE'] = '/content/hf_cache'\n",
        "    !mkdir -p /content/hf_cache\n",
        "elif is_kaggle:\n",
        "    os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'\n",
        "    !mkdir -p /kaggle/working/hf_cache\n",
        "print('HF_DATASETS_CACHE:', os.environ.get('HF_DATASETS_CACHE'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure we are in Q1 dir and outputs exists\n",
        "import os, pathlib\n",
        "Q1_DIR = os.getcwd()\n",
        "print('Working dir:', Q1_DIR)\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 1: Dataset Preparation & Testing\n",
        "# ===============================================\n",
        "print(\"🚀 TASK 1: Testing Dataset Loading...\")\n",
        "\n",
        "# Test CIFAR-10 dataset loading from HuggingFace\n",
        "!python -c \"from src.dataset import CIFAR10HFDataset; ds = CIFAR10HFDataset(split='train[:100]'); print(f'✅ Dataset loaded! Shape: {ds[0][0].shape}, Label: {ds[0][1]}, Classes: 10')\"\n",
        "\n",
        "# Test model creation\n",
        "!python -c \"from src.model import SimpleCNN; model = SimpleCNN(); print(f'✅ Model created with {sum(p.numel() for p in model.parameters()):,} parameters')\"\n",
        "\n",
        "print(\"✅ TASK 1 COMPLETE: Dataset and model verified!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 2: Build CNN and Train with Training Curves\n",
        "# ===============================================\n",
        "print(\"🧠 TASK 2: Training CNN with Training Curves...\")\n",
        "\n",
        "# Train baseline model (balanced epochs for good performance + reasonable time)\n",
        "!python -m src.train --epochs 25 --batch_size 128 --lr 0.001 --num_layers 3 --base_filters 32 --outdir outputs\n",
        "\n",
        "# Display training curve\n",
        "from IPython.display import Image, display\n",
        "print(\"📈 Training Loss Curve:\")\n",
        "display(Image('outputs/loss_curve.png'))\n",
        "\n",
        "print(\"✅ TASK 2 COMPLETE: CNN trained with training curves!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 3: Model Evaluation with Confusion Matrix & Metrics\n",
        "# ===============================================\n",
        "print(\"📊 TASK 3: Evaluating Model Performance...\")\n",
        "\n",
        "# Evaluate the trained model on test data\n",
        "!python -m src.evaluate --model_path outputs/best_model.pt --batch_size 128 --num_layers 3 --base_filters 32 --outdir outputs\n",
        "\n",
        "# Display results\n",
        "from IPython.display import Image, display\n",
        "import pandas as pd\n",
        "\n",
        "print(\"🎯 Confusion Matrix:\")\n",
        "display(Image('outputs/confusion_matrix.png'))\n",
        "\n",
        "print(\"📋 Performance Metrics:\")\n",
        "metrics = pd.read_csv('outputs/metrics.csv')\n",
        "display(metrics)\n",
        "\n",
        "print(\"📊 Per-Class Performance:\")\n",
        "per_class = pd.read_csv('outputs/per_class_metrics.csv')\n",
        "display(per_class.head(10))\n",
        "\n",
        "print(\"✅ TASK 3 COMPLETE: Model evaluated with confusion matrix and metrics!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 4: Feature Map Visualization & Analysis\n",
        "# ===============================================\n",
        "print(\"🔍 TASK 4: Extracting and Visualizing Feature Maps...\")\n",
        "\n",
        "# Extract and visualize feature maps from different layers\n",
        "!python -m src.visualize_features --model_path outputs/best_model.pt --batch_size 128 --num_layers 3 --base_filters 32 --outdir outputs --num_samples 6\n",
        "\n",
        "# Display feature maps\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "print(\"🎨 Feature Maps from Different Convolutional Layers:\")\n",
        "display(Image('outputs/feature_maps.png'))\n",
        "\n",
        "print(\"🧠 Layer-wise Feature Analysis:\")\n",
        "if os.path.exists('outputs/feature_analysis.txt'):\n",
        "    with open('outputs/feature_analysis.txt', 'r') as f:\n",
        "        print(f.read())\n",
        "\n",
        "print(\"✅ TASK 4 COMPLETE: Feature maps visualized and analyzed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 5: Hyperparameter Ablation Study\n",
        "# ===============================================\n",
        "print(\"⚡ TASK 5: Running Comprehensive Ablation Study...\")\n",
        "print(\"Testing: Learning Rate, Batch Size, Conv Filters, Number of Layers\")\n",
        "\n",
        "# Run ablation study (balanced epochs for thorough comparison)\n",
        "!python -m src.ablation_study --outdir outputs --epochs 15\n",
        "\n",
        "# Display results\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"📊 Ablation Study Results:\")\n",
        "ablation_results = pd.read_csv('outputs/ablation_results.csv')\n",
        "display(ablation_results)\n",
        "\n",
        "print(\"🏆 Best Hyperparameter Configurations:\")\n",
        "if os.path.exists('outputs/best_ablation_configs.json'):\n",
        "    import json\n",
        "    with open('outputs/best_ablation_configs.json', 'r') as f:\n",
        "        best_configs = json.load(f)\n",
        "    for exp_type, config in best_configs.items():\n",
        "        print(f\"{exp_type}: {config}\")\n",
        "\n",
        "print(\"✅ TASK 5 COMPLETE: Ablation study finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 6: Optimal Model Training & Performance Comparison\n",
        "# ===============================================\n",
        "print(\"🏆 TASK 6: Training Optimal Model and Comparing Performance...\")\n",
        "\n",
        "# Train model with optimal hyperparameters (more epochs for best performance)\n",
        "!python -m src.train_optimal --epochs 30 --best_config_file outputs/best_ablation_configs.json --outdir outputs\n",
        "\n",
        "# Evaluate optimal model\n",
        "!python -m src.evaluate --model_path outputs/optimal_model.pt --batch_size 128 --outdir outputs\n",
        "\n",
        "# Display comparison results\n",
        "import pandas as pd\n",
        "from IPython.display import Image, display\n",
        "\n",
        "print(\"📈 Optimal Model Training Curves:\")\n",
        "if os.path.exists('outputs/optimal_training_curves.png'):\n",
        "    display(Image('outputs/optimal_training_curves.png'))\n",
        "\n",
        "print(\"⚖️ Model Performance Comparison:\")\n",
        "if os.path.exists('outputs/model_comparison.csv'):\n",
        "    comparison = pd.read_csv('outputs/model_comparison.csv')\n",
        "    display(comparison)\n",
        "\n",
        "print(\"✅ TASK 6 COMPLETE: Optimal model trained and compared!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# SUMMARY & RESULTS OVERVIEW\n",
        "# ===============================================\n",
        "print(\"📋 FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# List all generated files\n",
        "print(\"📁 Generated Files:\")\n",
        "!ls -la outputs/\n",
        "\n",
        "# Show key metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "if os.path.exists('outputs/metrics.csv'):\n",
        "    print(\"\\n🎯 Final Model Performance:\")\n",
        "    metrics = pd.read_csv('outputs/metrics.csv')\n",
        "    display(metrics)\n",
        "\n",
        "if os.path.exists('outputs/model_comparison.csv'):\n",
        "    print(\"\\n⚖️ Baseline vs Optimal Model:\")\n",
        "    comparison = pd.read_csv('outputs/model_comparison.csv')\n",
        "    display(comparison)\n",
        "\n",
        "if os.path.exists('outputs/ablation_results.csv'):\n",
        "    print(\"\\n📊 Best Hyperparameters Found:\")\n",
        "    ablation = pd.read_csv('outputs/ablation_results.csv')\n",
        "    best_row = ablation.loc[ablation['accuracy'].idxmax()]\n",
        "    print(f\"Best Configuration: {best_row.to_dict()}\")\n",
        "\n",
        "print(\"\\n🎉 ALL TASKS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"✅ Dataset loaded and preprocessed\")\n",
        "print(\"✅ CNN trained with training curves\")\n",
        "print(\"✅ Model evaluated with confusion matrix\")\n",
        "print(\"✅ Feature maps visualized and analyzed\")\n",
        "print(\"✅ Hyperparameter ablation study completed\")\n",
        "print(\"✅ Optimal model trained and compared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# DOWNLOAD / SAVE ALL RESULTS - Q1_OUTPUTS\n",
        "# ===============================================\n",
        "print(\"📦 Preparing Q1_OUTPUTS for download/save...\")\n",
        "\n",
        "# Show what files we generated\n",
        "print(\"📁 Generated Files:\")\n",
        "!ls -la outputs/\n",
        "\n",
        "# Create single comprehensive ZIP file in the right place\n",
        "zip_path = '/content/Q1_OUTPUTS.zip' if is_colab else ('/kaggle/working/Q1_OUTPUTS.zip' if is_kaggle else 'Q1_OUTPUTS.zip')\n",
        "print(\"\\n🗜️ Creating:\", zip_path)\n",
        "!zip -r \"$zip_path\" outputs/ -x \"*.pyc\" \"*__pycache__*\"\n",
        "\n",
        "if is_colab:\n",
        "    from google.colab import files\n",
        "    print(\"\\n⬇️ Downloading Q1_OUTPUTS.zip to your local machine...\")\n",
        "    files.download(zip_path)\n",
        "elif is_kaggle:\n",
        "    from IPython.display import FileLink, display\n",
        "    print(\"\\nOn Kaggle, the zip is saved at:\", zip_path)\n",
        "    print(\"Use the right sidebar > Data > Output to download after the run finishes.\")\n",
        "    try:\n",
        "        display(FileLink(zip_path))\n",
        "    except Exception as e:\n",
        "        print('FileLink display failed:', e)\n",
        "else:\n",
        "    print(\"Zip created at:\", zip_path)\n",
        "\n",
        "print(\"\\n📋 Your Q1_OUTPUTS.zip contains:\")\n",
        "print(\"🔹 All model checkpoints (.pt files)\")\n",
        "print(\"🔹 Training curves and loss plots\")\n",
        "print(\"🔹 Confusion matrix visualization\")\n",
        "print(\"🔹 Performance metrics tables\")\n",
        "print(\"🔹 Feature map visualizations\")\n",
        "print(\"🔹 Complete ablation study results\")\n",
        "print(\"🔹 Model comparison data\")\n",
        "print(\"🔹 All generated analyses and plots\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXTRACT AND ANALYZE RESULTS LOCALLY\n",
        "# ===============================================\n",
        "print(\"📁 Instructions for using your Q1_OUTPUTS.zip:\")\n",
        "print(\"\\n1. 📂 Go to your Downloads folder\")\n",
        "print(\"2. 🗜️ Extract Q1_OUTPUTS.zip\")\n",
        "print(\"3. 📊 Open the 'outputs' folder\")\n",
        "print(\"4. 🔍 Explore your results:\")\n",
        "\n",
        "print(\"\\n   📈 Training Analysis:\")\n",
        "print(\"   - loss_curve.png - Training progress\")\n",
        "print(\"   - optimal_training_curves.png - Best model training\")\n",
        "\n",
        "print(\"\\n   🎯 Model Performance:\")\n",
        "print(\"   - confusion_matrix.png - Classification results\")\n",
        "print(\"   - metrics.csv - Accuracy, precision, recall, F1\")\n",
        "print(\"   - per_class_metrics.csv - Per-class performance\")\n",
        "\n",
        "print(\"\\n   🔬 Feature Analysis:\")\n",
        "print(\"   - feature_maps.png - What CNN layers learned\")\n",
        "print(\"   - feature_analysis.txt - Layer analysis\")\n",
        "\n",
        "print(\"\\n   ⚡ Hyperparameter Study:\")\n",
        "print(\"   - ablation_results.csv - All tested configurations\")\n",
        "print(\"   - best_ablation_configs.json - Optimal settings\")\n",
        "\n",
        "print(\"\\n   ⚖️ Model Comparison:\")\n",
        "print(\"   - model_comparison.csv - Baseline vs Optimal\")\n",
        "\n",
        "print(\"\\n   🤖 Model Files:\")\n",
        "print(\"   - best_model.pt - Baseline trained model\")\n",
        "print(\"   - optimal_model.pt - Best hyperparameter model\")\n",
        "\n",
        "print(\"\\n🎉 All 6 tasks completed successfully!\")\n",
        "print(\"🚀 Your complete Q1 analysis is ready for review!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
