{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Q1: CIFAR-10 CNN - Colab Runner\n",
        "\n",
        "This notebook sets up GPU, installs dependencies, runs ablation, trains the optimal model, evaluates on test set, visualizes feature maps, and zips outputs for download.\n",
        "\n",
        "- Make sure to pick GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "- Choose ONE of the three project setup paths below: Google Drive, GitHub clone, or Zip upload.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('Enable GPU in Runtime > Change runtime type')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Project setup: choose one path below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect environment (Colab vs Kaggle) and set base paths\n",
        "import os, sys, glob\n",
        "\n",
        "is_colab = 'google.colab' in sys.modules\n",
        "is_kaggle = bool(os.environ.get('KAGGLE_URL_BASE')) or os.path.exists('/kaggle/input')\n",
        "\n",
        "print('Environment:','Kaggle' if is_kaggle else ('Colab' if is_colab else 'Local'))\n",
        "\n",
        "BASE_DIR = '/content' if is_colab else ('/kaggle/working' if is_kaggle else os.getcwd())\n",
        "print('Base dir:', BASE_DIR)\n",
        "\n",
        "# Helper to try to locate the project automatically on Kaggle\n",
        "PROJECT_DIR = None\n",
        "Q1_DIR = None\n",
        "if is_kaggle:\n",
        "    candidates = []\n",
        "    # Prefer anything in /kaggle/working first\n",
        "    candidates += [p for p in [\n",
        "        '/kaggle/working/genai_A1',\n",
        "        '/kaggle/working/GenAi',\n",
        "        '/kaggle/working/genai',\n",
        "    ] if os.path.exists(p)]\n",
        "    # Search mounted input datasets for a folder that contains Q1/src\n",
        "    for root in glob.glob('/kaggle/input/*', recursive=False):\n",
        "        for p in [root, os.path.join(root, 'genai_A1'), os.path.join(root, 'GenAi')]:\n",
        "            q1 = os.path.join(p, 'Q1')\n",
        "            if os.path.exists(os.path.join(q1, 'src')):\n",
        "                candidates.append(p)\n",
        "    # Pick the shortest valid candidate\n",
        "    if candidates:\n",
        "        PROJECT_DIR = sorted(candidates, key=len)[0]\n",
        "        Q1_DIR = os.path.join(PROJECT_DIR, 'Q1')\n",
        "        print('Auto-detected PROJECT_DIR:', PROJECT_DIR)\n",
        "        print('Q1_DIR:', Q1_DIR)\n",
        "        os.chdir(Q1_DIR)\n",
        "        os.environ['PYTHONPATH'] = Q1_DIR\n",
        "        !pwd && ls -la\n",
        "    else:\n",
        "        print('Could not auto-detect project under /kaggle/input or /kaggle/working.')\n",
        "        print('If you attached a dataset, ensure it contains genai_A1/Q1 or GenAi/Q1.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle setup: copy project from /kaggle/input to /kaggle/working if needed\n",
        "import os, shutil, glob\n",
        "\n",
        "if is_kaggle:\n",
        "    def find_project_roots():\n",
        "        roots = []\n",
        "        # common names under working\n",
        "        for p in ['/kaggle/working/genai_A1', '/kaggle/working/GenAi', '/kaggle/working/genai']:\n",
        "            if os.path.exists(os.path.join(p, 'Q1', 'src')):\n",
        "                roots.append(p)\n",
        "        # search input datasets\n",
        "        for root in glob.glob('/kaggle/input/*', recursive=False):\n",
        "            for name in ['genai_A1', 'GenAi', 'genai']:\n",
        "                p = os.path.join(root, name)\n",
        "                if os.path.exists(os.path.join(p, 'Q1', 'src')):\n",
        "                    roots.append(p)\n",
        "            # Also allow when the dataset root itself contains Q1\n",
        "            if os.path.exists(os.path.join(root, 'Q1', 'src')):\n",
        "                roots.append(root)\n",
        "        return roots\n",
        "\n",
        "    roots = find_project_roots()\n",
        "    print('Detected project candidates:', roots)\n",
        "\n",
        "    # Choose a source under /kaggle/input if present; else prefer working\n",
        "    src = next((r for r in roots if r.startswith('/kaggle/input/')), None) or (roots[0] if roots else None)\n",
        "    if src is None:\n",
        "        print('No project found. Attach a Dataset with the repository and re-run.')\n",
        "    else:\n",
        "        dst = '/kaggle/working/genai_A1' if os.path.basename(src).lower() != 'genai_a1' else '/kaggle/working/genai_A1'\n",
        "        print('Source:', src)\n",
        "        print('Destination:', dst)\n",
        "        if not os.path.exists(dst):\n",
        "            print('Copying project to working...')\n",
        "            shutil.copytree(src, dst)\n",
        "        else:\n",
        "            print('Project already exists in working. Updating files (if any)...')\n",
        "            # minimal sync: copy src/Q1 over dst/Q1\n",
        "            for item in ['Q1', 'requirements.txt']:\n",
        "                s = os.path.join(src, item)\n",
        "                d = os.path.join(dst, item)\n",
        "                if os.path.exists(s):\n",
        "                    if os.path.isdir(s):\n",
        "                        shutil.copytree(s, d, dirs_exist_ok=True)\n",
        "                    else:\n",
        "                        shutil.copy2(s, d)\n",
        "        PROJECT_DIR = dst\n",
        "        Q1_DIR = os.path.join(PROJECT_DIR, 'Q1')\n",
        "        os.chdir(Q1_DIR)\n",
        "        os.environ['PYTHONPATH'] = Q1_DIR\n",
        "        print('Using PROJECT_DIR:', PROJECT_DIR)\n",
        "        print('Using Q1_DIR:', Q1_DIR)\n",
        "        !pwd && ls -la\n",
        "else:\n",
        "    print('Kaggle setup skipped (not running on Kaggle).')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Mount Google Drive and use existing project in Drive (Colab only)\n",
        "import os\n",
        "if is_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    PROJECT_DIR = '/content/drive/MyDrive/your_project_path/genai_A1'  # <-- EDIT THIS\n",
        "    Q1_DIR = os.path.join(PROJECT_DIR, 'Q1')\n",
        "    %cd $Q1_DIR\n",
        "    os.environ['PYTHONPATH'] = Q1_DIR\n",
        "    !pwd && ls -la\n",
        "else:\n",
        "    print('Skipping Google Drive mount (not Colab).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option B: Clone from GitHub (set path by environment)\n",
        "import os\n",
        "base = '/content' if is_colab else ('/kaggle/working' if is_kaggle else os.getcwd())\n",
        "%cd $base\n",
        "!git clone https://github.com/your/repo.git genai_A1\n",
        "%cd $base/genai_A1/Q1\n",
        "PROJECT_DIR = f\"{base}/genai_A1\"\n",
        "Q1_DIR = f\"{PROJECT_DIR}/Q1\"\n",
        "os.environ['PYTHONPATH'] = Q1_DIR\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option C: Upload or attach a ZIP and unzip to base dir\n",
        "import os\n",
        "base = '/content' if is_colab else ('/kaggle/working' if is_kaggle else os.getcwd())\n",
        "\n",
        "if is_colab:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # upload your project zip\n",
        "    zip_name = list(uploaded.keys())[0]\n",
        "    !unzip -o \"$zip_name\" -d $base\n",
        "else:\n",
        "    print('On Kaggle: attach a Dataset containing the repo ZIP or folder. If you have a local zip in working, set zip_name accordingly and run unzip manually:')\n",
        "    print('!unzip -o \"/kaggle/working/your_zip.zip\" -d /kaggle/working')\n",
        "\n",
        "%cd $base/genai_A1/Q1\n",
        "PROJECT_DIR = f\"{base}/genai_A1\"\n",
        "Q1_DIR = f\"{PROJECT_DIR}/Q1\"\n",
        "os.environ['PYTHONPATH'] = Q1_DIR\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "base = '/content' if is_colab else ('/kaggle/working' if is_kaggle else os.getcwd())\n",
        "req_path = f\"{base}/genai_A1/requirements.txt\"\n",
        "print('Using requirements:', req_path)\n",
        "%pip install -U pip\n",
        "%pip install -r \"$req_path\"\n",
        "\n",
        "# Optional: Speed up HF datasets cache\n",
        "import os\n",
        "if is_colab:\n",
        "    os.environ['HF_DATASETS_CACHE'] = '/content/hf_cache'\n",
        "    !mkdir -p /content/hf_cache\n",
        "elif is_kaggle:\n",
        "    os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'\n",
        "    !mkdir -p /kaggle/working/hf_cache\n",
        "print('HF_DATASETS_CACHE:', os.environ.get('HF_DATASETS_CACHE'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure we are in Q1 dir and outputs exists\n",
        "import os, pathlib\n",
        "Q1_DIR = os.getcwd()\n",
        "print('Working dir:', Q1_DIR)\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 1: Dataset Preparation & Testing\n",
        "# ===============================================\n",
        "print(\"üöÄ TASK 1: Testing Dataset Loading...\")\n",
        "\n",
        "# Test CIFAR-10 dataset loading from HuggingFace\n",
        "!python -c \"from src.dataset import CIFAR10HFDataset; ds = CIFAR10HFDataset(split='train[:100]'); print(f'‚úÖ Dataset loaded! Shape: {ds[0][0].shape}, Label: {ds[0][1]}, Classes: 10')\"\n",
        "\n",
        "# Test model creation\n",
        "!python -c \"from src.model import SimpleCNN; model = SimpleCNN(); print(f'‚úÖ Model created with {sum(p.numel() for p in model.parameters()):,} parameters')\"\n",
        "\n",
        "print(\"‚úÖ TASK 1 COMPLETE: Dataset and model verified!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 2: Build CNN and Train with Training Curves\n",
        "# ===============================================\n",
        "print(\"üß† TASK 2: Training CNN with Training Curves...\")\n",
        "\n",
        "# Train baseline model (balanced epochs for good performance + reasonable time)\n",
        "!python -m src.train --epochs 25 --batch_size 128 --lr 0.001 --num_layers 3 --base_filters 32 --outdir outputs\n",
        "\n",
        "# Display training curve\n",
        "from IPython.display import Image, display\n",
        "print(\"üìà Training Loss Curve:\")\n",
        "display(Image('outputs/loss_curve.png'))\n",
        "\n",
        "print(\"‚úÖ TASK 2 COMPLETE: CNN trained with training curves!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 3: Model Evaluation with Confusion Matrix & Metrics\n",
        "# ===============================================\n",
        "print(\"üìä TASK 3: Evaluating Model Performance...\")\n",
        "\n",
        "# Evaluate the trained model on test data\n",
        "!python -m src.evaluate --model_path outputs/best_model.pt --batch_size 128 --num_layers 3 --base_filters 32 --outdir outputs\n",
        "\n",
        "# Display results\n",
        "from IPython.display import Image, display\n",
        "import pandas as pd\n",
        "\n",
        "print(\"üéØ Confusion Matrix:\")\n",
        "display(Image('outputs/confusion_matrix.png'))\n",
        "\n",
        "print(\"üìã Performance Metrics:\")\n",
        "metrics = pd.read_csv('outputs/metrics.csv')\n",
        "display(metrics)\n",
        "\n",
        "print(\"üìä Per-Class Performance:\")\n",
        "per_class = pd.read_csv('outputs/per_class_metrics.csv')\n",
        "display(per_class.head(10))\n",
        "\n",
        "print(\"‚úÖ TASK 3 COMPLETE: Model evaluated with confusion matrix and metrics!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 4: Feature Map Visualization & Analysis\n",
        "# ===============================================\n",
        "print(\"üîç TASK 4: Extracting and Visualizing Feature Maps...\")\n",
        "\n",
        "# Extract and visualize feature maps from different layers\n",
        "!python -m src.visualize_features --model_path outputs/best_model.pt --batch_size 128 --num_layers 3 --base_filters 32 --outdir outputs --num_samples 6\n",
        "\n",
        "# Display feature maps\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "print(\"üé® Feature Maps from Different Convolutional Layers:\")\n",
        "display(Image('outputs/feature_maps.png'))\n",
        "\n",
        "print(\"üß† Layer-wise Feature Analysis:\")\n",
        "if os.path.exists('outputs/feature_analysis.txt'):\n",
        "    with open('outputs/feature_analysis.txt', 'r') as f:\n",
        "        print(f.read())\n",
        "\n",
        "print(\"‚úÖ TASK 4 COMPLETE: Feature maps visualized and analyzed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 5: Hyperparameter Ablation Study\n",
        "# ===============================================\n",
        "print(\"‚ö° TASK 5: Running Comprehensive Ablation Study...\")\n",
        "print(\"Testing: Learning Rate, Batch Size, Conv Filters, Number of Layers\")\n",
        "\n",
        "# Run ablation study (balanced epochs for thorough comparison)\n",
        "!python -m src.ablation_study --outdir outputs --epochs 15\n",
        "\n",
        "# Display results\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"üìä Ablation Study Results:\")\n",
        "ablation_results = pd.read_csv('outputs/ablation_results.csv')\n",
        "display(ablation_results)\n",
        "\n",
        "print(\"üèÜ Best Hyperparameter Configurations:\")\n",
        "if os.path.exists('outputs/best_ablation_configs.json'):\n",
        "    import json\n",
        "    with open('outputs/best_ablation_configs.json', 'r') as f:\n",
        "        best_configs = json.load(f)\n",
        "    for exp_type, config in best_configs.items():\n",
        "        print(f\"{exp_type}: {config}\")\n",
        "\n",
        "print(\"‚úÖ TASK 5 COMPLETE: Ablation study finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 6: Optimal Model Training & Performance Comparison\n",
        "# ===============================================\n",
        "print(\"üèÜ TASK 6: Training Optimal Model and Comparing Performance...\")\n",
        "\n",
        "# Train model with optimal hyperparameters (more epochs for best performance)\n",
        "!python -m src.train_optimal --epochs 30 --best_config_file outputs/best_ablation_configs.json --outdir outputs\n",
        "\n",
        "# Evaluate optimal model\n",
        "!python -m src.evaluate --model_path outputs/optimal_model.pt --batch_size 128 --outdir outputs\n",
        "\n",
        "# Display comparison results\n",
        "import pandas as pd\n",
        "from IPython.display import Image, display\n",
        "\n",
        "print(\"üìà Optimal Model Training Curves:\")\n",
        "if os.path.exists('outputs/optimal_training_curves.png'):\n",
        "    display(Image('outputs/optimal_training_curves.png'))\n",
        "\n",
        "print(\"‚öñÔ∏è Model Performance Comparison:\")\n",
        "if os.path.exists('outputs/model_comparison.csv'):\n",
        "    comparison = pd.read_csv('outputs/model_comparison.csv')\n",
        "    display(comparison)\n",
        "\n",
        "print(\"‚úÖ TASK 6 COMPLETE: Optimal model trained and compared!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# SUMMARY & RESULTS OVERVIEW\n",
        "# ===============================================\n",
        "print(\"üìã FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# List all generated files\n",
        "print(\"üìÅ Generated Files:\")\n",
        "!ls -la outputs/\n",
        "\n",
        "# Show key metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "if os.path.exists('outputs/metrics.csv'):\n",
        "    print(\"\\nüéØ Final Model Performance:\")\n",
        "    metrics = pd.read_csv('outputs/metrics.csv')\n",
        "    display(metrics)\n",
        "\n",
        "if os.path.exists('outputs/model_comparison.csv'):\n",
        "    print(\"\\n‚öñÔ∏è Baseline vs Optimal Model:\")\n",
        "    comparison = pd.read_csv('outputs/model_comparison.csv')\n",
        "    display(comparison)\n",
        "\n",
        "if os.path.exists('outputs/ablation_results.csv'):\n",
        "    print(\"\\nüìä Best Hyperparameters Found:\")\n",
        "    ablation = pd.read_csv('outputs/ablation_results.csv')\n",
        "    best_row = ablation.loc[ablation['accuracy'].idxmax()]\n",
        "    print(f\"Best Configuration: {best_row.to_dict()}\")\n",
        "\n",
        "print(\"\\nüéâ ALL TASKS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"‚úÖ Dataset loaded and preprocessed\")\n",
        "print(\"‚úÖ CNN trained with training curves\")\n",
        "print(\"‚úÖ Model evaluated with confusion matrix\")\n",
        "print(\"‚úÖ Feature maps visualized and analyzed\")\n",
        "print(\"‚úÖ Hyperparameter ablation study completed\")\n",
        "print(\"‚úÖ Optimal model trained and compared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# DOWNLOAD / SAVE ALL RESULTS - Q1_OUTPUTS\n",
        "# ===============================================\n",
        "print(\"üì¶ Preparing Q1_OUTPUTS for download/save...\")\n",
        "\n",
        "# Show what files we generated\n",
        "print(\"üìÅ Generated Files:\")\n",
        "!ls -la outputs/\n",
        "\n",
        "# Create single comprehensive ZIP file in the right place\n",
        "zip_path = '/content/Q1_OUTPUTS.zip' if is_colab else ('/kaggle/working/Q1_OUTPUTS.zip' if is_kaggle else 'Q1_OUTPUTS.zip')\n",
        "print(\"\\nüóúÔ∏è Creating:\", zip_path)\n",
        "!zip -r \"$zip_path\" outputs/ -x \"*.pyc\" \"*__pycache__*\"\n",
        "\n",
        "if is_colab:\n",
        "    from google.colab import files\n",
        "    print(\"\\n‚¨áÔ∏è Downloading Q1_OUTPUTS.zip to your local machine...\")\n",
        "    files.download(zip_path)\n",
        "elif is_kaggle:\n",
        "    from IPython.display import FileLink, display\n",
        "    print(\"\\nOn Kaggle, the zip is saved at:\", zip_path)\n",
        "    print(\"Use the right sidebar > Data > Output to download after the run finishes.\")\n",
        "    try:\n",
        "        display(FileLink(zip_path))\n",
        "    except Exception as e:\n",
        "        print('FileLink display failed:', e)\n",
        "else:\n",
        "    print(\"Zip created at:\", zip_path)\n",
        "\n",
        "print(\"\\nüìã Your Q1_OUTPUTS.zip contains:\")\n",
        "print(\"üîπ All model checkpoints (.pt files)\")\n",
        "print(\"üîπ Training curves and loss plots\")\n",
        "print(\"üîπ Confusion matrix visualization\")\n",
        "print(\"üîπ Performance metrics tables\")\n",
        "print(\"üîπ Feature map visualizations\")\n",
        "print(\"üîπ Complete ablation study results\")\n",
        "print(\"üîπ Model comparison data\")\n",
        "print(\"üîπ All generated analyses and plots\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXTRACT AND ANALYZE RESULTS LOCALLY\n",
        "# ===============================================\n",
        "print(\"üìÅ Instructions for using your Q1_OUTPUTS.zip:\")\n",
        "print(\"\\n1. üìÇ Go to your Downloads folder\")\n",
        "print(\"2. üóúÔ∏è Extract Q1_OUTPUTS.zip\")\n",
        "print(\"3. üìä Open the 'outputs' folder\")\n",
        "print(\"4. üîç Explore your results:\")\n",
        "\n",
        "print(\"\\n   üìà Training Analysis:\")\n",
        "print(\"   - loss_curve.png - Training progress\")\n",
        "print(\"   - optimal_training_curves.png - Best model training\")\n",
        "\n",
        "print(\"\\n   üéØ Model Performance:\")\n",
        "print(\"   - confusion_matrix.png - Classification results\")\n",
        "print(\"   - metrics.csv - Accuracy, precision, recall, F1\")\n",
        "print(\"   - per_class_metrics.csv - Per-class performance\")\n",
        "\n",
        "print(\"\\n   üî¨ Feature Analysis:\")\n",
        "print(\"   - feature_maps.png - What CNN layers learned\")\n",
        "print(\"   - feature_analysis.txt - Layer analysis\")\n",
        "\n",
        "print(\"\\n   ‚ö° Hyperparameter Study:\")\n",
        "print(\"   - ablation_results.csv - All tested configurations\")\n",
        "print(\"   - best_ablation_configs.json - Optimal settings\")\n",
        "\n",
        "print(\"\\n   ‚öñÔ∏è Model Comparison:\")\n",
        "print(\"   - model_comparison.csv - Baseline vs Optimal\")\n",
        "\n",
        "print(\"\\n   ü§ñ Model Files:\")\n",
        "print(\"   - best_model.pt - Baseline trained model\")\n",
        "print(\"   - optimal_model.pt - Best hyperparameter model\")\n",
        "\n",
        "print(\"\\nüéâ All 6 tasks completed successfully!\")\n",
        "print(\"üöÄ Your complete Q1 analysis is ready for review!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
