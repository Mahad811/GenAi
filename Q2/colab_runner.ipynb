{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Q2: Shakespeare RNN - Colab Runner\n",
        "\n",
        "This notebook sets up GPU, installs dependencies, runs ablation study, trains the optimal model, evaluates performance, generates text samples, and zips outputs for download.\n",
        "\n",
        "- Make sure to pick GPU: Runtime â†’ Change runtime type â†’ GPU\n",
        "- Choose ONE of the three project setup paths below: Google Drive, GitHub clone, or Zip upload.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('Enable GPU in Runtime > Change runtime type')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Project setup: choose one path below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Mount Google Drive and use existing project in Drive (edit the path)\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "PROJECT_DIR = '/content/drive/MyDrive/your_project_path/genai_A1'  # <-- EDIT THIS\n",
        "Q2_DIR = os.path.join(PROJECT_DIR, 'Q2')\n",
        "\n",
        "%cd $Q2_DIR\n",
        "os.environ['PYTHONPATH'] = Q2_DIR\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option B: Clone from GitHub (replace with your repo URL)\n",
        "import os\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/your/repo.git genai_A1\n",
        "%cd /content/genai_A1/Q2\n",
        "os.environ['PYTHONPATH'] = '/content/genai_A1/Q2'\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option C: Upload a ZIP and unzip to /content\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()  # upload your project zip\n",
        "zip_name = list(uploaded.keys())[0]\n",
        "!unzip -o \"$zip_name\" -d /content\n",
        "%cd /content/genai_A1/Q2\n",
        "os.environ['PYTHONPATH'] = '/content/genai_A1/Q2'\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -U pip\n",
        "%pip install -r /content/genai_A1/requirements.txt\n",
        "\n",
        "# Optional: Speed up HF datasets cache\n",
        "import os\n",
        "os.environ['HF_DATASETS_CACHE'] = '/content/hf_cache'\n",
        "!mkdir -p /content/hf_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure we are in Q2 dir and outputs exists\n",
        "import os, pathlib\n",
        "Q2_DIR = os.getcwd()\n",
        "print('Working dir:', Q2_DIR)\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 1: Dataset Loading and Preprocessing\n",
        "# ===============================================\n",
        "print(\"ğŸš€ TASK 1: Loading and Preprocessing Shakespeare Dataset...\")\n",
        "\n",
        "# Test dataset loading from HuggingFace\n",
        "!python -c \"from src.dataset import ShakespeareDataset; ds = ShakespeareDataset(split='train', seq_len=50); print(f'âœ… Dataset loaded! Vocab size: {ds.vocab_size}, Sequences: {len(ds)}')\"\n",
        "\n",
        "# Test model creation\n",
        "!python -c \"from src.model import ShakespeareRNN; model = ShakespeareRNN(vocab_size=100, embedding_dim=128, hidden_size=256); print(f'âœ… RNN model created with {sum(p.numel() for p in model.parameters()):,} parameters')\"\n",
        "\n",
        "print(\"âœ… TASK 1 COMPLETE: Dataset loaded and preprocessed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 2: RNN Model Implementation and Training\n",
        "# ===============================================\n",
        "print(\"ğŸ§  TASK 2: Training RNN Model with Custom Embeddings...\")\n",
        "\n",
        "# Train baseline RNN model (balanced epochs for good performance)\n",
        "!python -m src.train --epochs 20 --batch_size 64 --seq_len 100 --lr 0.001 --embedding_dim 128 --hidden_size 256 --num_layers 2 --rnn_type LSTM --outdir outputs\n",
        "\n",
        "# Display training curves\n",
        "from IPython.display import Image, display\n",
        "print(\"ğŸ“ˆ Training and Validation Curves:\")\n",
        "display(Image('outputs/training_curves.png'))\n",
        "\n",
        "print(\"âœ… TASK 2 COMPLETE: RNN model trained with custom embeddings!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 3: Text Generation with Seed Phrases\n",
        "# ===============================================\n",
        "print(\"ğŸ“ TASK 3: Generating Text with Seed Phrases...\")\n",
        "\n",
        "# Generate text with various seed phrases\n",
        "seed_phrases = [\n",
        "    \"To be or not to\",\n",
        "    \"Once upon a time\",\n",
        "    \"The quick brown fox\",\n",
        "    \"All the world's a\",\n",
        "    \"In fair Verona\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ­ Generating Shakespeare-style text with seed phrases:\")\n",
        "for i, seed in enumerate(seed_phrases, 1):\n",
        "    print(f\"\\n{i}. Seed: '{seed}'\")\n",
        "    !python -c \"\n",
        "from src.model import ShakespeareRNN\n",
        "from src.utils import generate_text\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load model and generate\n",
        "try:\n",
        "    checkpoint = torch.load('outputs/best_model.pt', map_location='cpu')\n",
        "    vocab_info = checkpoint['vocab_info']\n",
        "    model_info = checkpoint['model_info']\n",
        "    \n",
        "    model = ShakespeareRNN(\n",
        "        vocab_size=model_info['vocab_size'],\n",
        "        embedding_dim=model_info['embedding_dim'],\n",
        "        hidden_size=model_info['hidden_size'],\n",
        "        num_layers=model_info['num_layers'],\n",
        "        rnn_type=model_info['rnn_type']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    generated = generate_text(model, vocab_info, '$seed', max_length=50, temperature=0.8)\n",
        "    print(f'Generated: {generated}')\n",
        "except Exception as e:\n",
        "    print(f'Error: {e}')\n",
        "\"\n",
        "\n",
        "print(\"âœ… TASK 3 COMPLETE: Text generation with seed phrases!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 4: Model Performance Evaluation\n",
        "# ===============================================\n",
        "print(\"ğŸ“Š TASK 4: Evaluating Model Performance...\")\n",
        "\n",
        "# Comprehensive model evaluation with metrics\n",
        "!python -m src.evaluate --model_path outputs/best_model.pt --outdir outputs --num_samples 10 --max_length 100\n",
        "\n",
        "# Display evaluation results\n",
        "from IPython.display import Image, display\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"ğŸ“ˆ Model Performance Metrics:\")\n",
        "if os.path.exists('outputs/evaluation_metrics.csv'):\n",
        "    metrics = pd.read_csv('outputs/evaluation_metrics.csv')\n",
        "    display(metrics)\n",
        "\n",
        "print(\"\\nğŸ“Š Evaluation Visualizations:\")\n",
        "if os.path.exists('outputs/evaluation_results.png'):\n",
        "    display(Image('outputs/evaluation_results.png'))\n",
        "\n",
        "print(\"\\nğŸ“ Generated Text Samples:\")\n",
        "if os.path.exists('outputs/generated_texts.txt'):\n",
        "    with open('outputs/generated_texts.txt', 'r') as f:\n",
        "        generated_samples = f.read()\n",
        "        print(generated_samples[:1000] + \"...\" if len(generated_samples) > 1000 else generated_samples)\n",
        "\n",
        "print(\"âœ… TASK 4 COMPLETE: Model performance evaluated with perplexity and accuracy!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 5: Ablation Study - Model Component Analysis\n",
        "# ===============================================\n",
        "print(\"âš¡ TASK 5: Running Ablation Study...\")\n",
        "print(\"Testing: Hidden Size, Number of Layers, Dropout, RNN Type\")\n",
        "\n",
        "# Run comprehensive ablation study (balanced epochs for comparison)\n",
        "!python -m src.ablation_study --outdir outputs --epochs 12\n",
        "\n",
        "# Display ablation results\n",
        "import pandas as pd\n",
        "from IPython.display import Image, display\n",
        "\n",
        "print(\"ğŸ“Š Ablation Study Results:\")\n",
        "if os.path.exists('outputs/ablation_results.csv'):\n",
        "    ablation_results = pd.read_csv('outputs/ablation_results.csv')\n",
        "    display(ablation_results)\n",
        "\n",
        "print(\"ğŸ† Best Configuration Found:\")\n",
        "if os.path.exists('outputs/best_ablation_configs.json'):\n",
        "    import json\n",
        "    with open('outputs/best_ablation_configs.json', 'r') as f:\n",
        "        best_configs = json.load(f)\n",
        "    for component, config in best_configs.items():\n",
        "        print(f\"{component}: {config}\")\n",
        "\n",
        "print(\"ğŸ“ˆ Ablation Study Visualization:\")\n",
        "if os.path.exists('outputs/ablation_study_results.png'):\n",
        "    display(Image('outputs/ablation_study_results.png'))\n",
        "\n",
        "print(\"âœ… TASK 5 COMPLETE: Ablation study finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 6: Optimal Model Training & Performance Comparison\n",
        "# ===============================================\n",
        "print(\"ğŸ† TASK 6: Training Optimal Model with Best Configuration...\")\n",
        "\n",
        "# Train model with optimal hyperparameters from ablation study\n",
        "!python -m src.train_optimal --epochs 25 --best_config_file outputs/best_ablation_configs.json --outdir outputs\n",
        "\n",
        "# Compare baseline vs optimal model performance\n",
        "print(\"âš–ï¸ Model Performance Comparison:\")\n",
        "if os.path.exists('outputs/model_comparison.csv'):\n",
        "    comparison = pd.read_csv('outputs/model_comparison.csv')\n",
        "    display(comparison)\n",
        "\n",
        "print(\"ğŸ“ˆ Optimal Model Training Curves:\")\n",
        "if os.path.exists('outputs/optimal_training_curves.png'):\n",
        "    display(Image('outputs/optimal_training_curves.png'))\n",
        "\n",
        "# Final evaluation with optimal model\n",
        "!python -m src.evaluate --model_path outputs/optimal_model.pt --outdir outputs --num_samples 15 --max_length 100\n",
        "\n",
        "print(\"âœ… TASK 6 COMPLETE: Optimal model trained and compared!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# SUMMARY & RESULTS OVERVIEW\n",
        "# ===============================================\n",
        "print(\"ğŸ“‹ FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# List all generated files\n",
        "print(\"ğŸ“ Generated Files:\")\n",
        "!ls -la outputs/\n",
        "\n",
        "# Show key metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "if os.path.exists('outputs/evaluation_metrics.csv'):\n",
        "    print(\"\\nğŸ¯ Final Model Performance:\")\n",
        "    metrics = pd.read_csv('outputs/evaluation_metrics.csv')\n",
        "    display(metrics)\n",
        "\n",
        "if os.path.exists('outputs/model_comparison.csv'):\n",
        "    print(\"\\nâš–ï¸ Baseline vs Optimal Model:\")\n",
        "    comparison = pd.read_csv('outputs/model_comparison.csv')\n",
        "    display(comparison)\n",
        "\n",
        "if os.path.exists('outputs/ablation_results.csv'):\n",
        "    print(\"\\nğŸ“Š Best Hyperparameters Found:\")\n",
        "    ablation = pd.read_csv('outputs/ablation_results.csv')\n",
        "    best_row = ablation.loc[ablation['perplexity'].idxmin()]  # Lower perplexity is better\n",
        "    print(f\"Best Configuration: {best_row.to_dict()}\")\n",
        "\n",
        "print(\"\\nğŸ­ Sample Generated Text:\")\n",
        "if os.path.exists('outputs/generated_texts.txt'):\n",
        "    with open('outputs/generated_texts.txt', 'r') as f:\n",
        "        sample_text = f.read()[:500]  # Show first 500 characters\n",
        "        print(sample_text + \"...\" if len(sample_text) >= 500 else sample_text)\n",
        "\n",
        "print(\"\\nğŸ‰ ALL Q2 TASKS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"âœ… Dataset loaded and preprocessed\")\n",
        "print(\"âœ… RNN model implemented with custom embeddings\")\n",
        "print(\"âœ… Model trained with monitoring curves\")\n",
        "print(\"âœ… Text generation with seed phrases\")\n",
        "print(\"âœ… Performance evaluation with perplexity/accuracy\")\n",
        "print(\"âœ… Ablation study completed and compared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# DOWNLOAD ALL RESULTS - Q2_OUTPUTS\n",
        "# ===============================================\n",
        "print(\"ğŸ“¦ Preparing Q2_OUTPUTS for download...\")\n",
        "\n",
        "# Show what files we generated\n",
        "print(\"ğŸ“ Generated Files:\")\n",
        "!ls -la outputs/\n",
        "\n",
        "# Create single comprehensive ZIP file\n",
        "print(\"\\nğŸ—œï¸ Creating Q2_OUTPUTS.zip...\")\n",
        "!zip -r /content/Q2_OUTPUTS.zip outputs/ -x \"*.pyc\" \"*__pycache__*\"\n",
        "\n",
        "# Download the complete package\n",
        "from google.colab import files\n",
        "print(\"\\nâ¬‡ï¸ Downloading Q2_OUTPUTS.zip to your local CPU...\")\n",
        "files.download('/content/Q2_OUTPUTS.zip')\n",
        "\n",
        "print(\"\\nâœ… DOWNLOAD COMPLETE!\")\n",
        "print(\"ğŸ“¦ File downloaded: Q2_OUTPUTS.zip\")\n",
        "print(\"ğŸ’» Check your Downloads folder\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Your Q2_OUTPUTS.zip contains:\")\n",
        "print(\"ğŸ”¹ All trained RNN models (.pt files)\")\n",
        "print(\"ğŸ”¹ Training and validation curves\")\n",
        "print(\"ğŸ”¹ Generated Shakespeare text samples\")\n",
        "print(\"ğŸ”¹ Performance evaluation metrics\")\n",
        "print(\"ğŸ”¹ Perplexity and accuracy analysis\")\n",
        "print(\"ğŸ”¹ Complete ablation study results\")\n",
        "print(\"ğŸ”¹ Model comparison data\")\n",
        "print(\"ğŸ”¹ All generated analyses and plots\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# INTERACTIVE TEXT GENERATION (BONUS)\n",
        "# ===============================================\n",
        "print(\"ğŸ­ BONUS: Interactive Shakespeare Text Generation\")\n",
        "print(\"Try generating text with custom seed phrases!\")\n",
        "\n",
        "import torch\n",
        "from src.model import ShakespeareRNN\n",
        "from src.utils import generate_text\n",
        "\n",
        "try:\n",
        "    # Load the optimal model\n",
        "    checkpoint = torch.load('outputs/optimal_model.pt', map_location='cpu')\n",
        "    vocab_info = checkpoint['vocab_info']\n",
        "    model_info = checkpoint['model_info']\n",
        "\n",
        "    # Create and load model\n",
        "    model = ShakespeareRNN(\n",
        "        vocab_size=model_info['vocab_size'],\n",
        "        embedding_dim=model_info['embedding_dim'],\n",
        "        hidden_size=model_info['hidden_size'],\n",
        "        num_layers=model_info['num_layers'],\n",
        "        dropout=0.0,\n",
        "        rnn_type=model_info['rnn_type']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Interactive generation function\n",
        "    def generate_interactive(seed_text, max_length=50, temperature=1.0):\n",
        "        generated = generate_text(\n",
        "            model=model,\n",
        "            vocab_info=vocab_info,\n",
        "            seed_text=seed_text,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            device=torch.device('cpu')\n",
        "        )\n",
        "        return generated\n",
        "\n",
        "    # Example generations with different temperatures\n",
        "    print(\"\\nğŸ¯ Text Generation Examples with Different Creativity Levels:\")\n",
        "    \n",
        "    seed = \"To be or not to\"\n",
        "    temperatures = [0.5, 0.8, 1.2]\n",
        "    temp_names = [\"Conservative\", \"Balanced\", \"Creative\"]\n",
        "    \n",
        "    for temp, name in zip(temperatures, temp_names):\n",
        "        generated = generate_interactive(seed, max_length=60, temperature=temp)\n",
        "        print(f\"\\n{name} (temp={temp}):\")\n",
        "        print(f\"Seed: '{seed}'\")\n",
        "        print(f\"Generated: '{generated}'\")\n",
        "    \n",
        "    print(\"\\nâœ¨ Try your own seeds by modifying the code above!\")\n",
        "    print(\"ğŸ›ï¸ Adjust temperature: 0.5 (conservative) to 1.5 (very creative)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Interactive generation not available: {e}\")\n",
        "    print(\"Make sure the optimal model has been trained successfully.\")\n",
        "\n",
        "print(\"\\nğŸ‰ Q2 Shakespeare RNN Project Complete!\")\n",
        "print(\"ğŸ“– Your model can now generate Shakespeare-style text!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
