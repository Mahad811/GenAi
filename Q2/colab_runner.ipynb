{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Q2: Shakespeare RNN - Colab Runner\n",
        "\n",
        "This notebook sets up GPU, installs dependencies, runs ablation study, trains the optimal model, evaluates performance, generates text samples, and zips outputs for download.\n",
        "\n",
        "- Make sure to pick GPU: Runtime → Change runtime type → GPU\n",
        "- Choose ONE of the three project setup paths below: Google Drive, GitHub clone, or Zip upload.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('Enable GPU in Runtime > Change runtime type')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Project setup: choose one path below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Mount Google Drive and use existing project in Drive (edit the path)\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "PROJECT_DIR = '/content/drive/MyDrive/your_project_path/genai_A1'  # <-- EDIT THIS\n",
        "Q2_DIR = os.path.join(PROJECT_DIR, 'Q2')\n",
        "\n",
        "%cd $Q2_DIR\n",
        "os.environ['PYTHONPATH'] = Q2_DIR\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option B: Clone from GitHub (replace with your repo URL)\n",
        "import os\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/your/repo.git genai_A1\n",
        "%cd /content/genai_A1/Q2\n",
        "os.environ['PYTHONPATH'] = '/content/genai_A1/Q2'\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option C: Upload a ZIP and unzip to /content\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()  # upload your project zip\n",
        "zip_name = list(uploaded.keys())[0]\n",
        "!unzip -o \"$zip_name\" -d /content\n",
        "%cd /content/genai_A1/Q2\n",
        "os.environ['PYTHONPATH'] = '/content/genai_A1/Q2'\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -U pip\n",
        "%pip install -r /content/genai_A1/requirements.txt\n",
        "\n",
        "# Optional: Speed up HF datasets cache\n",
        "import os\n",
        "os.environ['HF_DATASETS_CACHE'] = '/content/hf_cache'\n",
        "!mkdir -p /content/hf_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure we are in Q2 dir and outputs exists\n",
        "import os, pathlib\n",
        "Q2_DIR = os.getcwd()\n",
        "print('Working dir:', Q2_DIR)\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 1: Dataset Loading and Preprocessing\n",
        "# ===============================================\n",
        "print(\"🚀 TASK 1: Loading and Preprocessing Shakespeare Dataset...\")\n",
        "\n",
        "# Test dataset loading from HuggingFace\n",
        "!python -c \"from src.dataset import ShakespeareDataset; ds = ShakespeareDataset(split='train', seq_len=50); print(f'✅ Dataset loaded! Vocab size: {ds.vocab_size}, Sequences: {len(ds)}')\"\n",
        "\n",
        "# Test model creation\n",
        "!python -c \"from src.model import ShakespeareRNN; model = ShakespeareRNN(vocab_size=100, embedding_dim=128, hidden_size=256); print(f'✅ RNN model created with {sum(p.numel() for p in model.parameters()):,} parameters')\"\n",
        "\n",
        "print(\"✅ TASK 1 COMPLETE: Dataset loaded and preprocessed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 2: RNN Model Implementation and Training\n",
        "# ===============================================\n",
        "print(\"🧠 TASK 2: Training RNN Model with Custom Embeddings...\")\n",
        "\n",
        "# Train baseline RNN model (balanced epochs for good performance)\n",
        "!python -m src.train --epochs 20 --batch_size 64 --seq_len 100 --lr 0.001 --embedding_dim 128 --hidden_size 256 --num_layers 2 --rnn_type LSTM --outdir outputs\n",
        "\n",
        "# Display training curves\n",
        "from IPython.display import Image, display\n",
        "print(\"📈 Training and Validation Curves:\")\n",
        "display(Image('outputs/training_curves.png'))\n",
        "\n",
        "print(\"✅ TASK 2 COMPLETE: RNN model trained with custom embeddings!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 3: Text Generation with Seed Phrases\n",
        "# ===============================================\n",
        "print(\"📝 TASK 3: Generating Text with Seed Phrases...\")\n",
        "\n",
        "# Generate text with various seed phrases\n",
        "seed_phrases = [\n",
        "    \"To be or not to\",\n",
        "    \"Once upon a time\",\n",
        "    \"The quick brown fox\",\n",
        "    \"All the world's a\",\n",
        "    \"In fair Verona\"\n",
        "]\n",
        "\n",
        "print(\"🎭 Generating Shakespeare-style text with seed phrases:\")\n",
        "for i, seed in enumerate(seed_phrases, 1):\n",
        "    print(f\"\\n{i}. Seed: '{seed}'\")\n",
        "    !python -c \"\n",
        "from src.model import ShakespeareRNN\n",
        "from src.utils import generate_text\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load model and generate\n",
        "try:\n",
        "    checkpoint = torch.load('outputs/best_model.pt', map_location='cpu')\n",
        "    vocab_info = checkpoint['vocab_info']\n",
        "    model_info = checkpoint['model_info']\n",
        "    \n",
        "    model = ShakespeareRNN(\n",
        "        vocab_size=model_info['vocab_size'],\n",
        "        embedding_dim=model_info['embedding_dim'],\n",
        "        hidden_size=model_info['hidden_size'],\n",
        "        num_layers=model_info['num_layers'],\n",
        "        rnn_type=model_info['rnn_type']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    generated = generate_text(model, vocab_info, '$seed', max_length=50, temperature=0.8)\n",
        "    print(f'Generated: {generated}')\n",
        "except Exception as e:\n",
        "    print(f'Error: {e}')\n",
        "\"\n",
        "\n",
        "print(\"✅ TASK 3 COMPLETE: Text generation with seed phrases!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 4: Model Performance Evaluation\n",
        "# ===============================================\n",
        "print(\"📊 TASK 4: Evaluating Model Performance...\")\n",
        "\n",
        "# Comprehensive model evaluation with metrics\n",
        "!python -m src.evaluate --model_path outputs/best_model.pt --outdir outputs --num_samples 10 --max_length 100\n",
        "\n",
        "# Display evaluation results\n",
        "from IPython.display import Image, display\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"📈 Model Performance Metrics:\")\n",
        "if os.path.exists('outputs/evaluation_metrics.csv'):\n",
        "    metrics = pd.read_csv('outputs/evaluation_metrics.csv')\n",
        "    display(metrics)\n",
        "\n",
        "print(\"\\n📊 Evaluation Visualizations:\")\n",
        "if os.path.exists('outputs/evaluation_results.png'):\n",
        "    display(Image('outputs/evaluation_results.png'))\n",
        "\n",
        "print(\"\\n📝 Generated Text Samples:\")\n",
        "if os.path.exists('outputs/generated_texts.txt'):\n",
        "    with open('outputs/generated_texts.txt', 'r') as f:\n",
        "        generated_samples = f.read()\n",
        "        print(generated_samples[:1000] + \"...\" if len(generated_samples) > 1000 else generated_samples)\n",
        "\n",
        "print(\"✅ TASK 4 COMPLETE: Model performance evaluated with perplexity and accuracy!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 5: Ablation Study - Model Component Analysis\n",
        "# ===============================================\n",
        "print(\"⚡ TASK 5: Running Ablation Study...\")\n",
        "print(\"Testing: Hidden Size, Number of Layers, Dropout, RNN Type\")\n",
        "\n",
        "# Run comprehensive ablation study (balanced epochs for comparison)\n",
        "!python -m src.ablation_study --outdir outputs --epochs 12\n",
        "\n",
        "# Display ablation results\n",
        "import pandas as pd\n",
        "from IPython.display import Image, display\n",
        "\n",
        "print(\"📊 Ablation Study Results:\")\n",
        "if os.path.exists('outputs/ablation_results.csv'):\n",
        "    ablation_results = pd.read_csv('outputs/ablation_results.csv')\n",
        "    display(ablation_results)\n",
        "\n",
        "print(\"🏆 Best Configuration Found:\")\n",
        "if os.path.exists('outputs/best_ablation_configs.json'):\n",
        "    import json\n",
        "    with open('outputs/best_ablation_configs.json', 'r') as f:\n",
        "        best_configs = json.load(f)\n",
        "    for component, config in best_configs.items():\n",
        "        print(f\"{component}: {config}\")\n",
        "\n",
        "print(\"📈 Ablation Study Visualization:\")\n",
        "if os.path.exists('outputs/ablation_study_results.png'):\n",
        "    display(Image('outputs/ablation_study_results.png'))\n",
        "\n",
        "print(\"✅ TASK 5 COMPLETE: Ablation study finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# TASK 6: Optimal Model Training & Performance Comparison\n",
        "# ===============================================\n",
        "print(\"🏆 TASK 6: Training Optimal Model with Best Configuration...\")\n",
        "\n",
        "# Train model with optimal hyperparameters from ablation study\n",
        "!python -m src.train_optimal --epochs 25 --best_config_file outputs/best_ablation_configs.json --outdir outputs\n",
        "\n",
        "# Compare baseline vs optimal model performance\n",
        "print(\"⚖️ Model Performance Comparison:\")\n",
        "if os.path.exists('outputs/model_comparison.csv'):\n",
        "    comparison = pd.read_csv('outputs/model_comparison.csv')\n",
        "    display(comparison)\n",
        "\n",
        "print(\"📈 Optimal Model Training Curves:\")\n",
        "if os.path.exists('outputs/optimal_training_curves.png'):\n",
        "    display(Image('outputs/optimal_training_curves.png'))\n",
        "\n",
        "# Final evaluation with optimal model\n",
        "!python -m src.evaluate --model_path outputs/optimal_model.pt --outdir outputs --num_samples 15 --max_length 100\n",
        "\n",
        "print(\"✅ TASK 6 COMPLETE: Optimal model trained and compared!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# SUMMARY & RESULTS OVERVIEW\n",
        "# ===============================================\n",
        "print(\"📋 FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# List all generated files\n",
        "print(\"📁 Generated Files:\")\n",
        "!ls -la outputs/\n",
        "\n",
        "# Show key metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "if os.path.exists('outputs/evaluation_metrics.csv'):\n",
        "    print(\"\\n🎯 Final Model Performance:\")\n",
        "    metrics = pd.read_csv('outputs/evaluation_metrics.csv')\n",
        "    display(metrics)\n",
        "\n",
        "if os.path.exists('outputs/model_comparison.csv'):\n",
        "    print(\"\\n⚖️ Baseline vs Optimal Model:\")\n",
        "    comparison = pd.read_csv('outputs/model_comparison.csv')\n",
        "    display(comparison)\n",
        "\n",
        "if os.path.exists('outputs/ablation_results.csv'):\n",
        "    print(\"\\n📊 Best Hyperparameters Found:\")\n",
        "    ablation = pd.read_csv('outputs/ablation_results.csv')\n",
        "    best_row = ablation.loc[ablation['perplexity'].idxmin()]  # Lower perplexity is better\n",
        "    print(f\"Best Configuration: {best_row.to_dict()}\")\n",
        "\n",
        "print(\"\\n🎭 Sample Generated Text:\")\n",
        "if os.path.exists('outputs/generated_texts.txt'):\n",
        "    with open('outputs/generated_texts.txt', 'r') as f:\n",
        "        sample_text = f.read()[:500]  # Show first 500 characters\n",
        "        print(sample_text + \"...\" if len(sample_text) >= 500 else sample_text)\n",
        "\n",
        "print(\"\\n🎉 ALL Q2 TASKS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"✅ Dataset loaded and preprocessed\")\n",
        "print(\"✅ RNN model implemented with custom embeddings\")\n",
        "print(\"✅ Model trained with monitoring curves\")\n",
        "print(\"✅ Text generation with seed phrases\")\n",
        "print(\"✅ Performance evaluation with perplexity/accuracy\")\n",
        "print(\"✅ Ablation study completed and compared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# DOWNLOAD ALL RESULTS - Q2_OUTPUTS\n",
        "# ===============================================\n",
        "print(\"📦 Preparing Q2_OUTPUTS for download...\")\n",
        "\n",
        "# Show what files we generated\n",
        "print(\"📁 Generated Files:\")\n",
        "!ls -la outputs/\n",
        "\n",
        "# Create single comprehensive ZIP file\n",
        "print(\"\\n🗜️ Creating Q2_OUTPUTS.zip...\")\n",
        "!zip -r /content/Q2_OUTPUTS.zip outputs/ -x \"*.pyc\" \"*__pycache__*\"\n",
        "\n",
        "# Download the complete package\n",
        "from google.colab import files\n",
        "print(\"\\n⬇️ Downloading Q2_OUTPUTS.zip to your local CPU...\")\n",
        "files.download('/content/Q2_OUTPUTS.zip')\n",
        "\n",
        "print(\"\\n✅ DOWNLOAD COMPLETE!\")\n",
        "print(\"📦 File downloaded: Q2_OUTPUTS.zip\")\n",
        "print(\"💻 Check your Downloads folder\")\n",
        "\n",
        "print(\"\\n📋 Your Q2_OUTPUTS.zip contains:\")\n",
        "print(\"🔹 All trained RNN models (.pt files)\")\n",
        "print(\"🔹 Training and validation curves\")\n",
        "print(\"🔹 Generated Shakespeare text samples\")\n",
        "print(\"🔹 Performance evaluation metrics\")\n",
        "print(\"🔹 Perplexity and accuracy analysis\")\n",
        "print(\"🔹 Complete ablation study results\")\n",
        "print(\"🔹 Model comparison data\")\n",
        "print(\"🔹 All generated analyses and plots\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# INTERACTIVE TEXT GENERATION (BONUS)\n",
        "# ===============================================\n",
        "print(\"🎭 BONUS: Interactive Shakespeare Text Generation\")\n",
        "print(\"Try generating text with custom seed phrases!\")\n",
        "\n",
        "import torch\n",
        "from src.model import ShakespeareRNN\n",
        "from src.utils import generate_text\n",
        "\n",
        "try:\n",
        "    # Load the optimal model\n",
        "    checkpoint = torch.load('outputs/optimal_model.pt', map_location='cpu')\n",
        "    vocab_info = checkpoint['vocab_info']\n",
        "    model_info = checkpoint['model_info']\n",
        "\n",
        "    # Create and load model\n",
        "    model = ShakespeareRNN(\n",
        "        vocab_size=model_info['vocab_size'],\n",
        "        embedding_dim=model_info['embedding_dim'],\n",
        "        hidden_size=model_info['hidden_size'],\n",
        "        num_layers=model_info['num_layers'],\n",
        "        dropout=0.0,\n",
        "        rnn_type=model_info['rnn_type']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Interactive generation function\n",
        "    def generate_interactive(seed_text, max_length=50, temperature=1.0):\n",
        "        generated = generate_text(\n",
        "            model=model,\n",
        "            vocab_info=vocab_info,\n",
        "            seed_text=seed_text,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            device=torch.device('cpu')\n",
        "        )\n",
        "        return generated\n",
        "\n",
        "    # Example generations with different temperatures\n",
        "    print(\"\\n🎯 Text Generation Examples with Different Creativity Levels:\")\n",
        "    \n",
        "    seed = \"To be or not to\"\n",
        "    temperatures = [0.5, 0.8, 1.2]\n",
        "    temp_names = [\"Conservative\", \"Balanced\", \"Creative\"]\n",
        "    \n",
        "    for temp, name in zip(temperatures, temp_names):\n",
        "        generated = generate_interactive(seed, max_length=60, temperature=temp)\n",
        "        print(f\"\\n{name} (temp={temp}):\")\n",
        "        print(f\"Seed: '{seed}'\")\n",
        "        print(f\"Generated: '{generated}'\")\n",
        "    \n",
        "    print(\"\\n✨ Try your own seeds by modifying the code above!\")\n",
        "    print(\"🎛️ Adjust temperature: 0.5 (conservative) to 1.5 (very creative)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Interactive generation not available: {e}\")\n",
        "    print(\"Make sure the optimal model has been trained successfully.\")\n",
        "\n",
        "print(\"\\n🎉 Q2 Shakespeare RNN Project Complete!\")\n",
        "print(\"📖 Your model can now generate Shakespeare-style text!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
