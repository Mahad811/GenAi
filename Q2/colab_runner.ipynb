{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Q2: Shakespeare RNN - Colab Runner\n",
        "\n",
        "This notebook sets up GPU, installs dependencies, runs ablation study, trains the optimal model, evaluates performance, generates text samples, and zips outputs for download.\n",
        "\n",
        "- Make sure to pick GPU: Runtime → Change runtime type → GPU\n",
        "- Choose ONE of the three project setup paths below: Google Drive, GitHub clone, or Zip upload.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('Enable GPU in Runtime > Change runtime type')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Project setup: choose one path below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Mount Google Drive and use existing project in Drive (edit the path)\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "PROJECT_DIR = '/content/drive/MyDrive/your_project_path/genai_A1'  # <-- EDIT THIS\n",
        "Q2_DIR = os.path.join(PROJECT_DIR, 'Q2')\n",
        "\n",
        "%cd $Q2_DIR\n",
        "os.environ['PYTHONPATH'] = Q2_DIR\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option B: Clone from GitHub (replace with your repo URL)\n",
        "import os\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/your/repo.git genai_A1\n",
        "%cd /content/genai_A1/Q2\n",
        "os.environ['PYTHONPATH'] = '/content/genai_A1/Q2'\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option C: Upload a ZIP and unzip to /content\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()  # upload your project zip\n",
        "zip_name = list(uploaded.keys())[0]\n",
        "!unzip -o \"$zip_name\" -d /content\n",
        "%cd /content/genai_A1/Q2\n",
        "os.environ['PYTHONPATH'] = '/content/genai_A1/Q2'\n",
        "!pwd && ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -U pip\n",
        "%pip install -r /content/genai_A1/requirements.txt\n",
        "\n",
        "# Optional: Speed up HF datasets cache\n",
        "import os\n",
        "os.environ['HF_DATASETS_CACHE'] = '/content/hf_cache'\n",
        "!mkdir -p /content/hf_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure we are in Q2 dir and outputs exists\n",
        "import os, pathlib\n",
        "Q2_DIR = os.getcwd()\n",
        "print('Working dir:', Q2_DIR)\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Run Ablation Study (adjust epochs for speed/quality)\n",
        "!python -m src.ablation_study --outdir outputs --epochs 5\n",
        "\n",
        "# Show ablation outputs\n",
        "!ls -la outputs | sed -n '1,120p'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Train with Optimal Hyperparameters and compare with Baseline\n",
        "!python -m src.train_optimal --outdir outputs --epochs 15 --best_config_file outputs/best_ablation_configs.json\n",
        "\n",
        "# Preview comparison results\n",
        "import pandas as pd\n",
        "cmp = pd.read_csv('outputs/model_comparison.csv')\n",
        "cmp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Evaluate Optimal Model and Generate Text Samples\n",
        "!python -m src.evaluate --model_path outputs/optimal_model.pt --outdir outputs --num_samples 10 --max_length 100\n",
        "\n",
        "# Show metrics\n",
        "import pandas as pd\n",
        "metrics = pd.read_csv('outputs/evaluation_metrics.csv')\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(metrics.to_string(index=False))\n",
        "\n",
        "# Show generated texts\n",
        "print(\"\\nGenerated Text Samples:\")\n",
        "with open('outputs/generated_texts.txt', 'r') as f:\n",
        "    print(f.read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Display Results and Visualizations\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "print('Ablation Study Results:')\n",
        "display(Image('outputs/ablation_study_results.png'))\n",
        "\n",
        "print('\\nOptimal vs Baseline Comparison:')\n",
        "display(Image('outputs/optimal_vs_baseline.png'))\n",
        "\n",
        "print('\\nEvaluation Results:')\n",
        "display(Image('outputs/evaluation_results.png'))\n",
        "\n",
        "# Show training history\n",
        "import pandas as pd\n",
        "history = pd.read_csv('outputs/training_history.csv')\n",
        "print(\"\\nTraining History:\")\n",
        "print(history.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Interactive Text Generation\n",
        "import torch\n",
        "from src.model import ShakespeareRNN\n",
        "from src.utils import generate_text\n",
        "\n",
        "# Load the optimal model\n",
        "checkpoint = torch.load('outputs/optimal_model.pt', map_location='cpu')\n",
        "vocab_info = checkpoint['vocab_info']\n",
        "model_info = checkpoint['model_info']\n",
        "\n",
        "# Create and load model\n",
        "model = ShakespeareRNN(\n",
        "    vocab_size=model_info['vocab_size'],\n",
        "    embedding_dim=model_info['embedding_dim'],\n",
        "    hidden_size=model_info['hidden_size'],\n",
        "    num_layers=model_info['num_layers'],\n",
        "    dropout=0.0,\n",
        "    rnn_type=model_info['rnn_type']\n",
        ")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# Interactive generation function\n",
        "def generate_interactive(seed_text, max_length=50, temperature=1.0):\n",
        "    generated = generate_text(\n",
        "        model=model,\n",
        "        vocab_info=vocab_info,\n",
        "        seed_text=seed_text,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        device=torch.device('cpu')\n",
        "    )\n",
        "    return generated\n",
        "\n",
        "# Example generations\n",
        "seeds = [\n",
        "    \"To be or not to\",\n",
        "    \"Once upon a time\",\n",
        "    \"The quick brown fox\",\n",
        "    \"In the beginning\",\n",
        "    \"All the world's a\"\n",
        "]\n",
        "\n",
        "print(\"Interactive Text Generation Examples:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, seed in enumerate(seeds):\n",
        "    generated = generate_interactive(seed, max_length=60, temperature=0.8)\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Seed: '{seed}'\")\n",
        "    print(f\"Generated: '{generated}'\")\n",
        "\n",
        "# You can also try your own seeds:\n",
        "# custom_seed = \"Your custom seed here\"\n",
        "# custom_generated = generate_interactive(custom_seed, max_length=100, temperature=1.0)\n",
        "# print(f\"\\nCustom Generation:\")\n",
        "# print(f\"Seed: '{custom_seed}'\")\n",
        "# print(f\"Generated: '{custom_generated}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Zip and Download all outputs\n",
        "!zip -r /content/Q2_outputs.zip outputs\n",
        "from google.colab import files\n",
        "files.download('/content/Q2_outputs.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
